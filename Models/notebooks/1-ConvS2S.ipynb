{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKc7XHK8cCUf"
      },
      "source": [
        "# Implementation of the ConvSeq2Seq architecture\n",
        "\n",
        "### This is the notebook used to train and test the SPARQL NMT ConvSeq2Seq model for the Knowledge Base-aware SPARQL Query Translation from Natural Language article\n",
        "\n",
        "Here are some interesting resources that helped us in our implementation:\n",
        "- https://github.com/bentrevett/pytorch-seq2seq\n",
        "- https://huggingface.co/spaces/gradio/HuBERT/blob/main/fairseq/models/transformer.py\n",
        "- https://github.com/pytorch/fairseq/blob/main/fairseq/models/fconv.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TACu-7wh4pQ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Please note that using [wandb](https://wandb.ai/site) is not required, but suggested as it provides a great way to track model perfomances during training. Install the package and set the const USE_WANDB to true if you wish to use it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAlgPuvZUlcv",
        "outputId": "2f6ec17e-976b-49ea-9d38-2f0dc915ff66"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install SPARQLWrapper\n",
        "!pip install torchtext==0.11.0\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w_Si7H4_hHl",
        "outputId": "f96ca8de-adac-424d-974d-2b19ed9342e6"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IHvt39H4xi-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from google.colab import files\n",
        "import wandb\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "from torchtext.legacy.data.dataset import TabularDataset\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "\n",
        "import json\n",
        "import random\n",
        "from google.colab import files, drive\n",
        "import numpy as np\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from typing import List, Dict, Tuple, DefaultDict, Union, Optional\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVce0VpQDLA6",
        "outputId": "cc317ae2-6661-4118-9b5d-2e6e88bec694"
      },
      "outputs": [],
      "source": [
        "# connect to google drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9YuxEjjwj5Z"
      },
      "source": [
        "## Consts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NybZg_jywkFJ"
      },
      "outputs": [],
      "source": [
        "EMB_DIM = 768\n",
        "HID_DIM = 512            # each conv. layer has 2 * hid_dim filters\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "BATCH_SIZE = 128\n",
        "DECODER_OUT_DIM = 512\n",
        "\n",
        "DATASET = 'dataset.json' # Name of the dataset to use, found in the Data repo under out_data\n",
        "USE_WANDBAI = True       # Use WANDBAI as a logging tool\n",
        "USE_COPY = False          # Train the models using a copy layer\n",
        "\n",
        "RANDOM = False           # Randomize the order of the entries\n",
        "LOWERCASE = False        # force lowercase for query and questions - \n",
        "                         # not recommended because it makes it very hard to go back to working SPARQL queries\n",
        "\n",
        "N_EPOCHS = 150          # Train for how many epochs\n",
        "MAX_LENGTH = 100        # After MAX_LENGTH tokens are predicted by the model without reaching <eos>, it will stop trying\n",
        "\n",
        "# For Training Gradients\n",
        "CLIP = 0.1\n",
        "\n",
        "# Model Train Parameters\n",
        "LEARNING_RATE = 3.5     # We found that a learning rate up to 3.5 could significantly \n",
        "                        # speed up the training without losing performance for the base versions of LC-QuAD\n",
        "\n",
        "TAGS = ['dbr:', 'dbo:', 'dbp:', 'dbc:', 'dct:', 'geo:', 'georss:']\n",
        "\n",
        "DatasetIterator = torchtext.legacy.data.Dataset\n",
        "\n",
        "MODEL_TYPE = \"cnns2s\"\n",
        "COPY_FLAG = \"copy\" if USE_COPY else \"no_copy\"\n",
        "DATASET_FAMILY = \"LC-QuAD\"\n",
        "DATASET_NAME = \"intermediary_question_original_data_2\" # DONT FORGET TO SET\n",
        "\n",
        "OUT_DRIVE_FOLDER_BASE = f\"/content/gdrive/MyDrive/PRETRAINED/{MODEL_TYPE}/{COPY_FLAG}/{DATASET_FAMILY}/{DATASET_NAME}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYwW2oyaw0Wh"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B-4a0NCmjyG"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6djRqSn-0iA"
      },
      "outputs": [],
      "source": [
        "# Translate a question into a sparql query\n",
        "def translate_sentence(tokens: Union[str, List[str]], \n",
        "                       src_field: Field,\n",
        "                       trg_field: Field,\n",
        "                       model: nn.Module,\n",
        "                       device: torch.device,\n",
        "                       max_len=MAX_LENGTH, \n",
        "                       predict_with_copy=USE_COPY) -> Tuple[List[str], int]:\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    # format as a list of strs\n",
        "    if isinstance(tokens, str):\n",
        "        tokens = tokenize_en(tokens)\n",
        "    if LOWERCASE:\n",
        "      tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # extend vocab with KB elems\n",
        "    if predict_with_copy:\n",
        "      resources_to_extend = extract_KB_elems(tokens)\n",
        "      KB_vocab = VocabDup(resources_to_extend, padding=0)\n",
        "\n",
        "      src_field = extend_vocabulary(src_field, KB_vocab)\n",
        "      trg_field = extend_vocabulary(trg_field, KB_vocab)\n",
        "\n",
        "    # add <sos> and <eos> delimiters\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    \n",
        "    # index sentence with extended src vocab\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_conved, encoder_combined, encoder_padding_mask = model.encoder(src_tensor.masked_fill(src_tensor >= model.encoder.tok_embedding.num_embeddings, 0))\n",
        "\n",
        "    # init empty query sentence with only <sos> token\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    # generate words\n",
        "    for i in range(max_len - 2):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor.masked_fill(trg_tensor >= model.decoder.tok_embedding.num_embeddings, 0), encoder_conved, encoder_combined, encoder_padding_mask)\n",
        "            if predict_with_copy:\n",
        "              output, attention = model.copy_layer(src_tensor, output, attention)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    trg_tokens = [trg_field.vocab.itos[i] if i < len(trg_field.vocab) else '<unk>' for i in trg_indexes]\n",
        "    return trg_tokens[1:], attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcnIWKwdWrXm"
      },
      "outputs": [],
      "source": [
        "# Translate a question into a sparql query\n",
        "def batch_translate(batch: torch.Tensor,\n",
        "                    trg_field: Field,\n",
        "                    model: nn.Module,\n",
        "                    device: torch.device,\n",
        "                    max_len=MAX_LENGTH, \n",
        "                    predict_with_copy=USE_COPY) -> Tuple[List[str], int]:\n",
        "\n",
        "      model.eval()\n",
        "    \n",
        "      src_tensor = batch.English\n",
        "      max_batch_len = max_len - 2\n",
        "        \n",
        "      with torch.no_grad():\n",
        "              encoder_conved, encoder_combined, encoder_padding_mask = model.encoder(src_tensor.masked_fill(src_tensor >= model.encoder.tok_embedding.num_embeddings, 0))\n",
        "\n",
        "      # init empty query sentence with only <sos> token\n",
        "      trg_tensor = torch.full((src_tensor.shape[0], 1), 2).to(device)\n",
        "\n",
        "      # generate words\n",
        "      for test in range(max_batch_len):\n",
        "          with torch.no_grad():\n",
        "              output, attention = model.decoder(trg_tensor.masked_fill(trg_tensor >= model.decoder.tok_embedding.num_embeddings, 0), encoder_conved, encoder_combined, encoder_padding_mask)\n",
        "              if predict_with_copy:\n",
        "                output, attention = model.copy_layer(src_tensor, output, attention)\n",
        "\n",
        "          output = output[:, -1, :]\n",
        "          pred_token = output.argmax(1)\n",
        "          pred_token = torch.unsqueeze(pred_token, dim=1)\n",
        "          trg_tensor = torch.cat((trg_tensor, pred_token), dim=1)\n",
        "    \n",
        "      # remove after eos\n",
        "      out = []\n",
        "      for sent in trg_tensor.cpu().numpy():\n",
        "\n",
        "        if trg_field.vocab.stoi[trg_field.eos_token] in sent:\n",
        "          try:\n",
        "            eos_id = np.where(sent == trg_field.vocab.stoi[trg_field.eos_token])[0][0]\n",
        "            sent = sent[:eos_id]\n",
        "\n",
        "          except:\n",
        "            pass\n",
        "\n",
        "        out.append([trg_field.vocab.itos[i] if i < len(trg_field.vocab) else '<unk>' for i in sent][1:])\n",
        "    \n",
        "      return out, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxkKce8uMKJK"
      },
      "outputs": [],
      "source": [
        "# Convert a batch of token ids to a batch of tokens (by Samuel)\n",
        "def get_batch_tokens(batch: torch.Tensor, field: Field) -> List[str]:\n",
        "    output_tokens = []\n",
        "    for pred_trg in batch:\n",
        "        eos_ids = (pred_trg == field.vocab.stoi[field.eos_token]).nonzero(as_tuple=True)[0]\n",
        "  \n",
        "        if eos_ids.nelement():\n",
        "            non_eos_tokens_ids = pred_trg[:eos_ids[0]]\n",
        "        else:\n",
        "            non_eos_tokens_ids = pred_trg\n",
        "        \n",
        "        output_tokens.append([field.vocab.itos[tok] if tok <= len(field.vocab) else '<unk>' for tok in non_eos_tokens_ids])\n",
        "    return output_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFOqtdhU8MHw"
      },
      "outputs": [],
      "source": [
        "# Calculate the BLEU score of our test set\n",
        "def calculate_bleu(data: DatasetIterator, \n",
        "                   src_field: Field, \n",
        "                   trg_field: Field, \n",
        "                   model: nn.Module, \n",
        "                   device: torch.device,\n",
        "                   predict_with_copy=USE_COPY, \n",
        "                   max_len=MAX_LENGTH) -> Tuple[float, float]:\n",
        "    print(\"Calculating BLEU score...\")\n",
        "    expected_trgs = []\n",
        "    pred_trgs = []\n",
        "    pred_copy_trgs = []\n",
        "\n",
        "    for datum in data:\n",
        "        src = vars(datum)['English']\n",
        "        trg = vars(datum)['SPARQL']\n",
        "\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, predict_with_copy=predict_with_copy)\n",
        "        if pred_trg[-1] is trg_field.eos_token:\n",
        "          pred_trg = pred_trg[:-1]\n",
        "        pred_trgs.append(pred_trg)\n",
        "        expected_trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, expected_trgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oGBuJHgMGHz"
      },
      "outputs": [],
      "source": [
        "# Calculate the BLEU score of our test set by batch\n",
        "def batch_bleu(iterator: DatasetIterator, \n",
        "               trg_field: Field, # use BASE_TRG for syntax!\n",
        "               model: nn.Module,\n",
        "               device: torch.device,\n",
        "               use_copy=USE_COPY) -> float:\n",
        "\n",
        "    bleu_preds = []\n",
        "    bleu_expected = []\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "      preds, _ = batch_translate(batch, trg_field, model, device, predict_with_copy=use_copy)\n",
        "      bleu_preds.extend(preds)\n",
        "      expected = get_batch_tokens(batch.SPARQL[:,1:-1], trg_field)\n",
        "      bleu_expected.extend(expected)\n",
        "\n",
        "    return bleu_score(bleu_preds, [[sent] for sent in bleu_expected])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW_DeKbvgu9E"
      },
      "outputs": [],
      "source": [
        "# Calculate the BLEU score of our test set - when using copy we want syntax\n",
        "def calculate_bleu_syntax(data: DatasetIterator, \n",
        "                          src_field: Field, \n",
        "                          trg_field: Field, \n",
        "                          model: nn.Module, \n",
        "                          device: torch.device, \n",
        "                          max_len=MAX_LENGTH) -> float:\n",
        "    print(\"Calculating BLEU score of syntax...\")\n",
        "    expected_trgs = []\n",
        "    expected_syntax = []\n",
        "    pred_trgs_syntax = []\n",
        "    pred_copy_trgs = []\n",
        "\n",
        "    for datum in data:\n",
        "        src = vars(datum)['English']\n",
        "        trg = vars(datum)['SPARQL']\n",
        "        pred_syntax, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, predict_with_copy=True)\n",
        "        if pred_syntax[-1] is trg_field.eos_token:\n",
        "          pred_syntax = pred_syntax[:-1]\n",
        "\n",
        "        pred_trgs_syntax.append(pred_syntax)\n",
        "\n",
        "        trg_syntax = ['<unk>' if token.startswith(tuple(TAGS)) else token for token in trg]\n",
        "        expected_syntax.append([trg_syntax])\n",
        "\n",
        "    return bleu_score(pred_trgs_syntax, expected_syntax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ijwOD9nLmD2"
      },
      "outputs": [],
      "source": [
        "# calculates some metrics on the test set\n",
        "def get_metrics(data: DatasetIterator,\n",
        "                test_entries: List[Dict], \n",
        "                src_field: Field, \n",
        "                trg_field: Field, \n",
        "                model: nn.Module,\n",
        "                device: torch.device, \n",
        "                max_len=MAX_LENGTH, \n",
        "                predict_with_copy=USE_COPY) -> Dict[str, float]:\n",
        "    print(\"Computing evaluation metrics...\")\n",
        "    expected_trgs = []\n",
        "    pred_trgs = []\n",
        "    error_report = []\n",
        "\n",
        "    for i, datum in enumerate(data):\n",
        "        src = vars(datum)['English']\n",
        "        trg = vars(datum)['SPARQL']\n",
        "\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, predict_with_copy)\n",
        "\n",
        "        pred_trg = pred_trg[:-1]\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        expected_trgs.append([trg])\n",
        "\n",
        "        error_entry = {\n",
        "            'id': test_entries[i]['_id'],\n",
        "            'template_id': test_entries[i]['template_id'],\n",
        "            'src': ' '.join(src),\n",
        "            'trg': ' '.join(trg),\n",
        "            'predicted': ' '.join(pred_trg),\n",
        "            'correct': trg == pred_trg\n",
        "        }\n",
        "        error_report.append(error_entry)\n",
        "\n",
        "    metrics = {}\n",
        "    nb_examples = len(expected_trgs)\n",
        "    metrics['bleu'] = bleu_score(pred_trgs, expected_trgs)\n",
        "    metrics['accuracy'] = sum([int(pred_trgs[i] == expected_trgs[i][0]) for i in range(nb_examples)])/nb_examples\n",
        "\n",
        "    pred_ngrams = [list(ngrams_iterator(pred, len(pred))) for pred in pred_trgs]\n",
        "    exp_ngrams = [list(ngrams_iterator(exp[0], len(exp[0]))) for exp in expected_trgs]\n",
        "\n",
        "    #https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460\n",
        "    n_commons = [len(set(pred_ngrams[i]) & set(exp_ngrams[i])) for i in range(nb_examples)]\n",
        "\n",
        "    recalls = [n_commons[i] / len(exp_ngrams[i]) for i in range(nb_examples)]\n",
        "    metrics['macro recall'] = sum(recalls) / len(recalls)\n",
        "\n",
        "    precisions = [n_commons[i] / len(pred_ngrams[i]) for i in range(nb_examples)]\n",
        "    metrics['macro precision'] = sum(precisions) / len(precisions)\n",
        "\n",
        "    metrics['f1 score'] = 2 * (metrics['macro precision'] * metrics['macro recall']) / (metrics['macro precision'] + metrics['macro recall'])\n",
        "\n",
        "    with open('out/error_report.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(error_report, f, indent=4)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylUzwqDDmn4t"
      },
      "source": [
        "### Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWZ-OARGzoyF"
      },
      "outputs": [],
      "source": [
        "# imitation of a torchtext.vocab.Vocab, basic structure needed to extend a torchtext Vocab\n",
        "class VocabDup:\n",
        "  def __init__(self, vocab: Union[Dict[int, str], List[str]], padding=0, base_vocab_size=0):\n",
        "    if type(vocab) is list:\n",
        "      self.make_vocab_from_list(vocab, padding)\n",
        "\n",
        "    elif type(vocab) is dict:\n",
        "      self.make_vocab_from_dict(vocab, base_vocab_size)\n",
        "    \n",
        "    else:\n",
        "      raise ValueError(\"Could not make a vocab from this structure\")\n",
        "\n",
        "\n",
        "  # Make vocab from a list (usually KB elem list) to use it to extend base vocabs\n",
        "  def make_vocab_from_list(self, word_list: List[str], padding=0) -> None:\n",
        "      word_list = list(set(word_list))\n",
        "      word_counter = Counter(word_list)\n",
        "      stoi = defaultdict(int)\n",
        "      itos = [None for _ in range(len(word_list) + padding)]\n",
        "\n",
        "      curr_idx = 0\n",
        "      # pad if necessary\n",
        "      for i in range(padding):\n",
        "          word = f'not_a_resource_{i}'\n",
        "        \n",
        "          stoi[word] = curr_idx\n",
        "          itos[curr_idx] = word\n",
        "          curr_idx+=1\n",
        "\n",
        "      # add KB elems\n",
        "      for word in word_counter:\n",
        "          stoi[word] = curr_idx\n",
        "          itos[curr_idx] = word\n",
        "          curr_idx+=1\n",
        "\n",
        "      self.freq = word_counter\n",
        "      self.itos = itos\n",
        "      self.stoi = stoi\n",
        "\n",
        "  # Make vocab from a dict (usually when loading the vocab files)\n",
        "  def make_vocab_from_dict(self, word_dict: Dict[int, str], base_vocab_size: int=0) -> None:\n",
        "      stoi = defaultdict(int)\n",
        "      base_vocab_size = len(word_dict.values()) if base_vocab_size < 1 else base_vocab_size\n",
        "      itos = [None for _ in range(base_vocab_size)]\n",
        "\n",
        "      for idx, word in word_dict.items():\n",
        "          if idx < base_vocab_size:\n",
        "              stoi[word] = idx\n",
        "              itos[idx] = word\n",
        "\n",
        "      word_counter = Counter(itos)\n",
        "\n",
        "      self.freq = word_counter\n",
        "      self.itos = itos\n",
        "      self.stoi = stoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7xMOOO8QISA"
      },
      "outputs": [],
      "source": [
        "# by samuel\n",
        "def hide_KB_elems(tokens: List[str], unk_token = '<unk>') -> List[str]:\n",
        "  return [unk_token if token.startswith(tuple(TAGS)) else token for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoZu1M7ocAH3"
      },
      "outputs": [],
      "source": [
        "# Extract KB elements from a tokenized sentence\n",
        "def extract_KB_elems(tokens: List[str]) -> List[str]:\n",
        "  removed_resources_en = [t for t in tokens if t.startswith(tuple(TAGS))]\n",
        "  return removed_resources_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t0Paaklzdnd"
      },
      "outputs": [],
      "source": [
        "# Split vocbaularies: keep only the base words of question and queries, and save all removed KB elems in another list\n",
        "def abstract_KB_elems(data) -> Tuple[Dict, Dict]:\n",
        "  base_vocabs = {'English': [], 'SPARQL': []}\n",
        "  kb_vocabs = {'English': [], 'SPARQL': []}\n",
        "\n",
        "  for example in data:\n",
        "    nl = example.English\n",
        "    sparql = example.SPARQL\n",
        "\n",
        "    # for nl\n",
        "    filtered_nl = [t for t in nl if not t.startswith(tuple(TAGS))]\n",
        "    removed_resources_nl = [t for t in nl if t.startswith(tuple(TAGS))]\n",
        "\n",
        "    # for sparql\n",
        "    filtered_sparql = [t for t in sparql if not t.startswith(tuple(TAGS))]\n",
        "    removed_resources_sparql = [t for t in sparql if t.startswith(tuple(TAGS))]\n",
        "\n",
        "    # keep separated by sentences\n",
        "    base_vocabs['English'].append(filtered_nl)\n",
        "    base_vocabs['SPARQL'].append(filtered_sparql)\n",
        "\n",
        "    # a single list of all KB elems\n",
        "    kb_vocabs['English'].extend(removed_resources_nl)\n",
        "    kb_vocabs['SPARQL'].extend(removed_resources_sparql)\n",
        "\n",
        "  return base_vocabs, kb_vocabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C3XYYfdbWsK"
      },
      "outputs": [],
      "source": [
        "# This function acts exactly like the PyTorch version, but using the PyTorch version Field.vocab.extend_vocabulary cause\n",
        "# some seriously weird bugs. Our best guess was that it caused collisions in the dict keys, but it is highly unlikely\n",
        "def extend_vocabulary(field: Field, extension: VocabDup) -> Field:\n",
        "    words = extension.itos\n",
        "    for w in words:\n",
        "        if w not in field.vocab.itos: # stoi does not work\n",
        "            field.vocab.itos.append(w)\n",
        "            field.vocab.stoi[w] = len(field.vocab.itos) - 1\n",
        "\n",
        "    return field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sG_RR-zmgQ0"
      },
      "outputs": [],
      "source": [
        "# It is possible that a query contains a KB elem that is in the KB vocab but not in the question (for example, LC-QuAD template ID 7)\n",
        "# In that case, we should replace KB elems that are not in BOTH the query and the question by unknown tokens (0)\n",
        "def fix_extended_vocab(src: List[List[int]], trg: List[List[int]], base_voc_limit_trg: int, unk_token = 0) -> List[List[int]]:\n",
        "  for i_s, sentence in enumerate(trg): # batch size\n",
        "    for i_t, token_idx in enumerate(trg[i_s]): # batch size\n",
        "      if token_idx >= base_voc_limit_trg and token_idx not in src[i_s]:\n",
        "        trg[i_s][i_t] = unk_token\n",
        "\n",
        "  return trg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IsW6VuXzIlv"
      },
      "outputs": [],
      "source": [
        "# Save vocab to reuse for inference\n",
        "def save_vocab(vocab: torchtext.vocab.Vocab, path: str) -> None:\n",
        "    with open(path, 'w', encoding='utf-8') as f:     \n",
        "        for token, index in vocab.stoi.items():\n",
        "            f.write(f'{index}\\t{token}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zReT0YM97IcZ"
      },
      "outputs": [],
      "source": [
        "# Read vocab files\n",
        "def read_vocab(path: str) -> Dict[int, str]:\n",
        "    voc = {}\n",
        "    i = 0\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read().splitlines()\n",
        "        for line in data:\n",
        "            index, token = line.split('\\t')\n",
        "            voc[i] = token\n",
        "            i += 1\n",
        "    return voc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeH21-kbmt_a"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS0MnbOknopx"
      },
      "outputs": [],
      "source": [
        "# Tokenize a question by splitting at spaces\n",
        "def tokenize_en(text: str) -> List[str]:\n",
        "    text = text.replace('?', ' ? ')\n",
        "    splitted = text.split()\n",
        "    return [w for w in splitted if len(w) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvsxS38ExRPs"
      },
      "outputs": [],
      "source": [
        "# Tokenize a query by splitting at spaces\n",
        "def tokenize_sparql(text: str) -> List[str]:\n",
        "    splitted = text.split()\n",
        "    return [w for w in splitted if len(w) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUEi1Fgrydez"
      },
      "outputs": [],
      "source": [
        "# Generate train, val and test sets\n",
        "def gen_train_test_val_sets(train_examples: List[str], \n",
        "                            valid_examples: List[str], \n",
        "                            test_examples: List[str], \n",
        "                            data_fields: List[Tuple[str, Field]]) -> Tuple[TabularDataset, TabularDataset, TabularDataset]:\n",
        "    train_set = pd.DataFrame(train_examples, columns=[\"English\", \"SPARQL\"])\n",
        "    valid_set = pd.DataFrame(valid_examples, columns=[\"English\", \"SPARQL\"])\n",
        "    test_set = pd.DataFrame(test_examples, columns=[\"English\", \"SPARQL\"])\n",
        "\n",
        "    train_set = pd.DataFrame(train_set, columns=[\"English\", \"SPARQL\"])\n",
        "    valid_set = pd.DataFrame(valid_set, columns=[\"English\", \"SPARQL\"])\n",
        "    test_set = pd.DataFrame(test_set, columns=[\"English\", \"SPARQL\"])\n",
        "\n",
        "    train_set.to_csv(\"train.csv\", index=False, header=None)\n",
        "    valid_set.to_csv(\"valid.csv\", index=False, header=None)\n",
        "    test_set.to_csv(\"test.csv\", index=False, header=None)\n",
        "\n",
        "    train_data, valid_data, test_data = torchtext.legacy.data.TabularDataset.splits(\n",
        "        path='./', train='train.csv', validation='valid.csv', test='test.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    return train_data, valid_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YEOoI9Eyen5"
      },
      "outputs": [],
      "source": [
        "# Generate the data fields used to encode the question-query pairs\n",
        "def gen_data_field() -> Tuple[Field, Field]:\n",
        "    SRC = Field(tokenize=tokenize_en,\n",
        "                init_token='<sos>',\n",
        "                eos_token='<eos>',\n",
        "                lower=LOWERCASE,\n",
        "                batch_first=True)\n",
        "\n",
        "    TRG = Field(tokenize=tokenize_sparql,\n",
        "                init_token='<sos>',\n",
        "                eos_token='<eos>',\n",
        "                lower=LOWERCASE,\n",
        "                batch_first=True)\n",
        "\n",
        "    return SRC, TRG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWtcm2AimxTp"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXNXyKuwxVKT"
      },
      "outputs": [],
      "source": [
        "# Initialize model weights\n",
        "def initialize_weights(m) -> None:\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stA1fVMYnsJg"
      },
      "outputs": [],
      "source": [
        "# Count number of parameters in the model\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2igmPMeo9Gx"
      },
      "outputs": [],
      "source": [
        "# Calculate epoch duration\n",
        "def epoch_time(start_time: float, end_time: float) -> Tuple[float, float]:\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDNbYijCoRUG"
      },
      "outputs": [],
      "source": [
        "# Training function of the model\n",
        "def train(model: nn.Module, \n",
        "          iterator: DatasetIterator, \n",
        "          optimizer: torch.optim.Optimizer, \n",
        "          criterion: nn.Module, clip: float, \n",
        "          use_copy=USE_COPY) -> float:\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "\n",
        "    for _, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.English\n",
        "        trg = batch.SPARQL\n",
        "        \n",
        "        if use_copy: \n",
        "          trg = fix_extended_vocab(src, trg, OUT_TRG_DIM)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        output_dim = output.shape[-1]\n",
        "           \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1) \n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += [loss.item()]\n",
        "\n",
        "    return epoch_loss[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETHixOZQxYpM"
      },
      "outputs": [],
      "source": [
        "# Eval function of the model\n",
        "def evaluate(model: nn.Module, \n",
        "             iterator: DatasetIterator, \n",
        "             criterion: nn.Module, \n",
        "             use_copy=USE_COPY) -> float:\n",
        "\n",
        "    model.eval()\n",
        "    epoch_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.English\n",
        "            trg = batch.SPARQL\n",
        "\n",
        "            if use_copy: \n",
        "              trg = fix_extended_vocab(src, trg, OUT_TRG_DIM)\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += [loss.item()]\n",
        "        \n",
        "    return epoch_loss[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rSsdV20pefs"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU6E4ByCSZ2c"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int, # SV (base source vocab length)\n",
        "                 emb_dim: int,   # E (embed)\n",
        "                 hid_dim: int,   # C (conv)\n",
        "                 dropout: float,\n",
        "                 device: torch.device,\n",
        "                 padding_idx: int,\n",
        "                 max_length=MAX_LENGTH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim, self.padding_idx)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim, self.padding_idx)\n",
        "\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        in_channels = hid_dim\n",
        "\n",
        "        self.projections = nn.ModuleList()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "        self.residuals = []\n",
        "\n",
        "        # Hardcoded for our purposes, but definitely could be changed/passed as a parameter\n",
        "        convolutions = [(hid_dim, 3, 1)] * 9 + [(2 * hid_dim, 3, 1)] * 4 + [(4 * hid_dim, 1, 1)] * 2\n",
        "        layer_in_channels = [in_channels]\n",
        "        \n",
        "        for (out_channels, kernel_size, residual) in convolutions:\n",
        "            \n",
        "            residual_dim = layer_in_channels[-residual]\n",
        "            self.projections.append(\n",
        "                nn.Linear(residual_dim, out_channels)\n",
        "                if residual_dim != out_channels\n",
        "                else None\n",
        "            )\n",
        "\n",
        "            self.convolutions.append(\n",
        "                nn.Conv1d(\n",
        "                    in_channels,\n",
        "                    out_channels * 2,\n",
        "                    kernel_size,\n",
        "                    padding = kernel_size // 2\n",
        "                )\n",
        "            )\n",
        "            self.residuals.append(residual)\n",
        "            in_channels = out_channels\n",
        "            layer_in_channels.append(out_channels)\n",
        "        \n",
        "        self.hid2emb = nn.Linear(in_channels, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # src = B x S x SV\n",
        "        batch_size = src.shape[0] # B\n",
        "        src_len = src.shape[1] # S (longest sentence in src batch)\n",
        "\n",
        "        pos = torch.arange(self.padding_idx + 1, src_len + 1 + self.padding_idx).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos = [0, 1, 2, 3, ..., src len - 1] -> B x S\n",
        "        encoder_padding_mask = src.eq(self.padding_idx)  # B x S\n",
        "\n",
        "        if not encoder_padding_mask.any():\n",
        "          encoder_padding_mask = None\n",
        "        else:\n",
        "          pos.masked_fill_(encoder_padding_mask, self.padding_idx)\n",
        "\n",
        "        # embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src) # B x S x E\n",
        "        pos_embedded = self.pos_embedding(pos) # B x S x E\n",
        "\n",
        "        # combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded) # B x S x E\n",
        "\n",
        "        # pass embedded through linear layer to convert from emb dim (E) to hid dim (C)\n",
        "        x = self.emb2hid(embedded) # B x S x C\n",
        "\n",
        "        # permute for convolutional layer\n",
        "        x = x.permute(0, 2, 1) # B x C x S\n",
        "\n",
        "        # convolutions\n",
        "        residuals = [x]\n",
        "        for proj, conv, res_layer in zip(\n",
        "            self.projections, self.convolutions, self.residuals\n",
        "        ):\n",
        "            if res_layer > 0:\n",
        "                residual = residuals[-res_layer]\n",
        "\n",
        "                residual = residual\n",
        "                if proj is not None:\n",
        "                  residual = proj(residual.permute(0,2,1))\n",
        "                  residual = residual.permute(0,2,1) # B x C x S\n",
        "            else:\n",
        "                residual = None\n",
        "\n",
        "            if encoder_padding_mask is not None:\n",
        "              x = x.masked_fill(encoder_padding_mask.unsqueeze(1), 0)\n",
        "\n",
        "            x = conv(self.dropout(x)) # B x C x S\n",
        "            x = F.glu(x, dim=1) # B x C x S\n",
        "\n",
        "            if residual is not None:\n",
        "              x = (x + residual) * math.sqrt(0.5)\n",
        "\n",
        "            residuals.append(x)\n",
        "\n",
        "        # permute and convert back to emb dim\n",
        "        x = self.hid2emb(x.permute(0, 2, 1)) # B x S x E\n",
        "\n",
        "        if encoder_padding_mask is not None:\n",
        "            x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)\n",
        "        \n",
        "        # Element-wise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (x + embedded) * math.sqrt(0.5)\n",
        "\n",
        "        # x -> B x S x E\n",
        "        # combined -> B x S x E\n",
        "        # encoder_paddding_mask -> # B x S\n",
        "\n",
        "        return x, combined, encoder_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wo_BMV25SLT"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, conv_channels: int, embed_dim: int, bmm=None):\n",
        "        super().__init__()\n",
        "        self.in_projection = nn.Linear(conv_channels, embed_dim) # C to E\n",
        "        self.out_projection = nn.Linear(embed_dim, conv_channels) # E to C\n",
        "        self.bmm = bmm if bmm is not None else torch.bmm\n",
        "\n",
        "    def forward(self, \n",
        "                x: torch.Tensor, \n",
        "                target_embedding: torch.Tensor, \n",
        "                encoder_out: torch.Tensor, \n",
        "                encoder_padding_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        residual = x\n",
        "\n",
        "        # x -> B x T x C\n",
        "        # target_embedding -> # B x T x E\n",
        "        # encoder_out[0] -> B x E x S\n",
        "        # encoder_out[1] -> B x S x E\n",
        "        # src_mask -> B x S\n",
        "\n",
        "        x = (self.in_projection(x) + target_embedding) * math.sqrt(0.5) # B x T x E\n",
        "        x = self.bmm(x, encoder_out[0]) # B x T x E\n",
        "\n",
        "        if encoder_padding_mask is not None:\n",
        "            x = (\n",
        "                x.float()\n",
        "                .masked_fill(encoder_padding_mask.unsqueeze(1), float(\"-inf\"))\n",
        "                .type_as(x)\n",
        "            ) # B x T x S\n",
        "\n",
        "        # softmax over last dim\n",
        "        sz = x.size()\n",
        "        x = F.softmax(x.view(sz[0] * sz[1], sz[2]), dim=1)\n",
        "        x = x.view(sz)\n",
        "        attn_scores = x # B x T x S\n",
        "\n",
        "        x = self.bmm(x, encoder_out[1]) # B x T x E\n",
        "\n",
        "        # scale attention output (respecting potentially different lengths)\n",
        "        s = encoder_out[1].size(1) # S\n",
        "\n",
        "        if encoder_padding_mask is None:\n",
        "            x = x * (s * math.sqrt(1.0 / s)) # B x T x E\n",
        "        else:\n",
        "            s = s - encoder_padding_mask.type_as(x).sum(\n",
        "                dim=1, keepdim=True\n",
        "            )  # exclude padding\n",
        "            s = s.unsqueeze(-1)\n",
        "            x = x * (s * s.rsqrt()) # B x T x E\n",
        "\n",
        "        # project back\n",
        "        x = (self.out_projection(x) + residual) * math.sqrt(0.5) # B x T x C\n",
        "\n",
        "        # attn_scores -> T x S\n",
        "        return x, attn_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oUZ4GGPSOdp"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_embeddings: int, # TV (base target vocab length)\n",
        "                 output_dim: int,     # O \n",
        "                 emb_dim: int,        # E\n",
        "                 hid_dim: int,\n",
        "                 dropout: float,\n",
        "                 device: torch.device,\n",
        "                 padding_idx: int,\n",
        "                 max_length=MAX_LENGTH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(num_embeddings, emb_dim, self.padding_idx)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim, self.padding_idx)\n",
        "\n",
        "        self.projections = nn.ModuleList()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "        self.attention = nn.ModuleList()\n",
        "        self.residuals = []\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        in_channels = hid_dim\n",
        "        layer_in_channels = [in_channels]\n",
        "\n",
        "        # Should be the same as in the encoder\n",
        "        convolutions = [(hid_dim, 3, 1)] * 9 + [(2*hid_dim, 3, 1)] * 4 + [(4*hid_dim, 1, 1)] * 2\n",
        "\n",
        "        for i, (out_channels, kernel_size, residual) in enumerate(convolutions):\n",
        "            residual_dim = layer_in_channels[-residual] # hid dim\n",
        "            self.projections.append(\n",
        "                nn.Linear(residual_dim, out_channels) \n",
        "                if residual_dim != out_channels\n",
        "                else None\n",
        "            )\n",
        "\n",
        "            self.convolutions.append(\n",
        "                nn.Conv1d(\n",
        "                    in_channels,\n",
        "                    out_channels * 2,\n",
        "                    kernel_size\n",
        "                )\n",
        "            )           \n",
        "\n",
        "            self.attention.append(\n",
        "                AttentionLayer(out_channels, emb_dim)\n",
        "            )\n",
        "\n",
        "            self.residuals.append(residual)\n",
        "            in_channels = out_channels\n",
        "            layer_in_channels.append(out_channels)\n",
        "\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(in_channels, output_dim)\n",
        "        self.fc_out = nn.Linear(output_dim, num_embeddings)\n",
        "\n",
        "\n",
        "    def forward(self, \n",
        "                trg: torch.Tensor, \n",
        "                encoder_conved: torch.Tensor, \n",
        "                encoder_combined: torch.Tensor, \n",
        "                src_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        # encoder_conved -> B x S x E\n",
        "        # encoder_combined -> B x S x E\n",
        "        # src_mask -> # B x S\n",
        "\n",
        "        batch_size = trg.shape[0] # B\n",
        "        trg_len = trg.shape[1] # T (longest sentence in targe batch)\n",
        "\n",
        "        encoder_conved = encoder_conved.permute(0, 2, 1) # B x E x S\n",
        "\n",
        "        # create position tensor\n",
        "        pos = torch.arange(self.padding_idx + 1, trg_len + 1 + self.padding_idx).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos = B x T\n",
        "\n",
        "        decoder_padding_mask = trg.eq(self.padding_idx)  # -> B x T\n",
        "        if not decoder_padding_mask.any():\n",
        "          decoder_padding_mask = None\n",
        "        else:\n",
        "          pos.masked_fill_(decoder_padding_mask, self.padding_idx)\n",
        "\n",
        "        # embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg) # B x T x E\n",
        "        pos_embedded = self.pos_embedding(pos) # B x T x E\n",
        "\n",
        "        # combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded) # B x T x E\n",
        "\n",
        "        # pass embedded through linear layer to go through emb dim (E) -> hid dim (C)\n",
        "        x = self.emb2hid(embedded) # B x T x C\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        residuals = [x]\n",
        "        for proj, conv, attention, res_layer in zip(\n",
        "            self.projections, self.convolutions, self.attention, self.residuals\n",
        "        ):\n",
        "            if res_layer > 0:\n",
        "                residual = residuals[-res_layer] # B x T x C\n",
        "                residual = residual if proj is None else proj(residual)  # B x T x C\n",
        "            else:\n",
        "                residual = None\n",
        "\n",
        "            hid_dim = x.shape[2] # C\n",
        "            x = self.dropout(x) # B x T x C\n",
        "            # we permute it here as opposed to beforer the conv in the encoder because of the attn layer\n",
        "            x = x.permute(0, 2, 1) # B x C x T\n",
        "\n",
        "            # K is kernel size\n",
        "            padding = torch.zeros(batch_size, hid_dim, conv.kernel_size[0] - 1) # B x C x K\n",
        "            padding = padding.fill_(self.padding_idx).to(self.device) # B x C x K\n",
        "            padded_x = torch.cat((padding, x), dim = 2) # B x C x [K + T]\n",
        "\n",
        "            x = conv(padded_x)     # B x C x [K + T]\n",
        "            x = F.glu(x, dim=1)    # B x C x [K + T]\n",
        "            x = x.permute(0, 2, 1) # B x [K + T] x C\n",
        "\n",
        "            if attention is not None:\n",
        "                attn, attn_scores = attention(\n",
        "                    x, embedded, (encoder_conved, encoder_combined), src_mask\n",
        "\n",
        "                )\n",
        "                \n",
        "            if residual is not None:\n",
        "              x = (attn + residual) * math.sqrt(0.5) # B x T x C\n",
        "\n",
        "            residuals.append(x)\n",
        "\n",
        "        x = self.hid2emb(x) # B x T x E\n",
        "        output = self.fc_out(self.dropout(x)) # B x T x O\n",
        "        # attn_scores -> S x T\n",
        "        return output, attn_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS48sx0oKKUf"
      },
      "outputs": [],
      "source": [
        "class CopyLayerVocabExtend(nn.Module):\n",
        "  def __init__(self, decoder: Decoder):\n",
        "    super().__init__()\n",
        "    self.switch = nn.Linear(decoder.tok_embedding.num_embeddings, 1)\n",
        "\n",
        "  def forward(self,\n",
        "              src: torch.Tensor, \n",
        "              output: torch.Tensor, \n",
        "              attention: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    p_pointer = torch.sigmoid(self.switch(output)) # prob copie vs gen\n",
        "  \n",
        "    if torch.max(src) + 1 > output.shape[-1]: # estce que source contient des oov? disons oov id 1490 vs taille de base 1000\n",
        "      extended = Variable(torch.zeros((output.shape[0], output.shape[1], torch.max(src) + 1 - output.shape[-1]))).to(output.device) # taille 490\n",
        "      output = torch.cat((output, extended), dim = 2) # size output + 490\n",
        "\n",
        "    output = ((1 - p_pointer) * F.softmax(output, dim = 2)).scatter_add(2, src.unsqueeze(1).repeat(1, output.shape[1], 1), p_pointer * attention) + 1e-10\n",
        "    return torch.log(output), attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc9340anSTb1"
      },
      "outputs": [],
      "source": [
        "class CNNSeq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder: Encoder, \n",
        "                 decoder: Decoder, \n",
        "                 copy_layer: Optional[CopyLayerVocabExtend]=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.switch = nn.Linear(decoder.tok_embedding.num_embeddings, 1)\n",
        "        self.copy_layer = copy_layer\n",
        "\n",
        "    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "      if self.copy_layer is None:\n",
        "        encoder_conved, encoder_combined, encoder_padding_mask = self.encoder(src)\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined, encoder_padding_mask)\n",
        "        return output, attention\n",
        "\n",
        "      else:\n",
        "        encoder_conved, encoder_combined, encoder_padding_mask = self.encoder(src.masked_fill(src >= self.encoder.tok_embedding.num_embeddings, 0))\n",
        "        output, attention = self.decoder(trg.masked_fill(trg >= self.decoder.tok_embedding.num_embeddings, 0), encoder_conved, encoder_combined, encoder_padding_mask)\n",
        "        output, attention = self.copy_layer(src, output, attention)\n",
        "        return output, attention\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bn40FNXx8hZ"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b6nejgbEE52"
      },
      "outputs": [],
      "source": [
        "iteration = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yljs_WmLEF-n"
      },
      "outputs": [],
      "source": [
        "# START RERUN HERE ===================================================================================================================================================================================================\n",
        "OUT_DRIVE_FOLDER = f'{OUT_DRIVE_FOLDER_BASE}/{iteration}'\n",
        "RUN_NAME = f'{MODEL_TYPE}_{DATASET_NAME}_{iteration}'\n",
        "GROUP_NAME = f'{MODEL_TYPE}_{DATASET_NAME}'\n",
        "WANDBAI_TAGS = [MODEL_TYPE, COPY_FLAG, DATASET_FAMILY]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x2K_6tuP_5s",
        "outputId": "9eb5f3c7-6564-4f80-80c8-8d8f660cc363"
      },
      "outputs": [],
      "source": [
        "iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdk8wqBSal1Y"
      },
      "outputs": [],
      "source": [
        "!mkdir out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwqIZB3ULu2-"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "with open(DATASET, 'r', encoding='utf-8') as f:\n",
        "  dataset = json.load(f)\n",
        "  \n",
        "if RANDOM:\n",
        "  random.shuffle(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b39tiINc5J4U"
      },
      "outputs": [],
      "source": [
        "# Split into sets\n",
        "test_entries = [entry for entry in dataset if entry['set'] == 'test']\n",
        "\n",
        "# You will have to manually change the dict keys depending on the version of LC-QuAD you want\n",
        "train_examples = [(entry['original_data']['lcquad']['intermediary_question'].lower().replace('<','').replace('>',''), entry['query']['interm_sparql']) for entry in dataset if entry['set'] == 'train']\n",
        "valid_examples = [(entry['original_data']['lcquad']['intermediary_question'].lower().replace('<','').replace('>',''), entry['query']['interm_sparql']) for entry in dataset if entry['set'] == 'valid']\n",
        "test_examples =  [(entry['original_data']['lcquad']['intermediary_question'].lower().replace('<','').replace('>',''), entry['query']['interm_sparql']) for entry in dataset if entry['set'] == 'test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEp55dgH3P4n"
      },
      "outputs": [],
      "source": [
        "# Init Fields\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SRC, TRG = gen_data_field()\n",
        "data_fields = [('English', SRC), ('SPARQL', TRG)]\n",
        "train_data, valid_data, test_data = gen_train_test_val_sets(train_examples, valid_examples, test_examples, data_fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apzMOtrI3T4Q"
      },
      "outputs": [],
      "source": [
        "# build base vocabs\n",
        "if USE_COPY:\n",
        "  vocab_data_train, removed_resources_train = abstract_KB_elems(train_data)\n",
        "  vocab_data_valid, removed_resources_valid = abstract_KB_elems(valid_data)\n",
        "  vocab_data_test, removed_resources_test = abstract_KB_elems(test_data)\n",
        "\n",
        "  vocab_data = {}\n",
        "  vocab_data['English'] = vocab_data_train['English'] + vocab_data_valid['English'] + vocab_data_test['English']\n",
        "  vocab_data['SPARQL'] = vocab_data_train['SPARQL'] + vocab_data_valid['SPARQL'] + vocab_data_test['SPARQL']\n",
        "  \n",
        "  SRC.build_vocab(vocab_data['English'], min_freq=1, max_size=None)\n",
        "  TRG.build_vocab(vocab_data['SPARQL'], min_freq=1, max_size=None)\n",
        "\n",
        "  removed_resources = {}\n",
        "  removed_resources['English'] = removed_resources_train['English'] + removed_resources_valid['English'] + removed_resources_test['English']\n",
        "  removed_resources['SPARQL'] = removed_resources_train['SPARQL'] + removed_resources_valid['SPARQL'] + removed_resources_test['SPARQL']\n",
        "\n",
        "  removed_resources_src = set(removed_resources['English'])\n",
        "  removed_resources_trg = set(removed_resources['SPARQL'])\n",
        "\n",
        "  removed_resources = removed_resources_src.union(removed_resources_trg)\n",
        "\n",
        "else:\n",
        "  SRC.build_vocab(train_data, min_freq=1)\n",
        "  TRG.build_vocab(train_data, min_freq=1)\n",
        "\n",
        "BASE_SRC = copy.deepcopy(SRC)\n",
        "BASE_TRG = copy.deepcopy(TRG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRLueg_J3ZV4",
        "outputId": "53bbdb56-6e9c-42d2-a16f-dfbb136e7e1d"
      },
      "outputs": [],
      "source": [
        "IN_SRC_DIM = len(SRC.vocab)\n",
        "OUT_TRG_DIM = len(TRG.vocab)\n",
        "print('BASE LENGTH SRC', IN_SRC_DIM)\n",
        "print('BASE LENGTH TRG', OUT_TRG_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpkQpw0njXBg",
        "outputId": "fb7c1724-3056-4d85-c8ba-3b6727bd5843"
      },
      "outputs": [],
      "source": [
        "# extend vocabs if use copy\n",
        "if USE_COPY:\n",
        "  KB_vocab_src_extension = VocabDup(list(removed_resources), padding=max(0, OUT_TRG_DIM - IN_SRC_DIM))\n",
        "  KB_vocab_trg_extension = VocabDup(list(removed_resources), padding=max(0, IN_SRC_DIM - OUT_TRG_DIM))\n",
        "\n",
        "  SRC.vocab.extend(KB_vocab_src_extension)   \n",
        "  TRG.vocab.extend(KB_vocab_trg_extension)\n",
        "\n",
        "  # very important!!!\n",
        "  for i in range(IN_SRC_DIM, len(SRC.vocab)):\n",
        "    assert SRC.vocab.itos[i] == TRG.vocab.itos[i]\n",
        "\n",
        "  assert len(SRC.vocab) == len(TRG.vocab)\n",
        "\n",
        "\n",
        "  print('BASE SRC VOCAB', len(BASE_SRC.vocab))\n",
        "  print('BASE TRG VOCAB', len(BASE_TRG.vocab))\n",
        "  print('EXTENDED SRC VOCAB', len(SRC.vocab))\n",
        "  print('EXTENDED TRG VOCAB', len(TRG.vocab))\n",
        "  print('KB VOCAB', len(SRC.vocab) - max(IN_SRC_DIM, OUT_TRG_DIM))\n",
        "else:\n",
        "  print(\"NOT USING COPY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxL0fK8Q3gyo"
      },
      "outputs": [],
      "source": [
        "# Make data into iterators\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort_key=lambda x: len(x.SPARQL)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "JPFSNdl2fijM",
        "outputId": "1ff6028a-6dad-40b4-c45b-eed042619489"
      },
      "outputs": [],
      "source": [
        "if USE_WANDBAI:\n",
        "  wandb_config = {\n",
        "    \"dataset_family\": DATASET_FAMILY,\n",
        "    \"dataset\": DATASET_NAME,\n",
        "    \"use_copy\": USE_COPY,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"hidden_dims\": HID_DIM,\n",
        "    \"encoder_dropout\": ENC_DROPOUT,\n",
        "    \"decoder_dropout\": DEC_DROPOUT,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"n_epochs\": N_EPOCHS,\n",
        "    \"clip\": CLIP,\n",
        "    \"groupe\": GROUP_NAME\n",
        "  }\n",
        "\n",
        "  wandb.init(project=\"final\", entity=\"rooose\", config=wandb_config, name=RUN_NAME, tags=WANDBAI_TAGS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f7-DsaL5HUh"
      },
      "outputs": [],
      "source": [
        "# generate and save config and vocabs files for inference\n",
        "INPUT_DIM = IN_SRC_DIM\n",
        "NUM_EMBEDDINGS = OUT_TRG_DIM\n",
        "\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "config = {\n",
        "      'INPUT_DIM': INPUT_DIM,\n",
        "      'NUM_EMBEDDINGS': NUM_EMBEDDINGS,\n",
        "      'EMB_DIM': EMB_DIM,\n",
        "      'HID_DIM': HID_DIM,\n",
        "      'USE_COPY': USE_COPY,\n",
        "      'ENCODER': {\n",
        "          'ENC_DROPOUT': ENC_DROPOUT\n",
        "      },\n",
        "      'DECODER': {\n",
        "          'DEC_DROPOUT': DEC_DROPOUT,\n",
        "          'OUT_DIM': DECODER_OUT_DIM,\n",
        "      },\n",
        "      'SRC_PAD_IDX': SRC.vocab.stoi[SRC.pad_token],\n",
        "      'TRG_PAD_IDX': TRG.vocab.stoi[TRG.pad_token]\n",
        "  }\n",
        "\n",
        "with open('out/config.json', 'w') as f:\n",
        "  json.dump(config, f)\n",
        "\n",
        "# saves the extended vocab, could be optimized\n",
        "save_vocab(SRC.vocab, 'out/src_vocab.field')\n",
        "save_vocab(TRG.vocab, 'out/trg_vocab.field')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5JpCBeZzNgj"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_DROPOUT, device, SRC.vocab.stoi[SRC.pad_token])\n",
        "dec = Decoder(NUM_EMBEDDINGS, DECODER_OUT_DIM, EMB_DIM, HID_DIM, DEC_DROPOUT, device, TRG.vocab.stoi[TRG.pad_token])\n",
        "copy_layer = CopyLayerVocabExtend(dec) if USE_COPY else None\n",
        "\n",
        "model = CNNSeq2Seq(enc, dec, copy_layer).to(device)\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9) \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX, label_smoothing=0.1)\n",
        "\n",
        "if USE_WANDBAI:\n",
        "  wandb.watch(model, log_freq = 20)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "bleu = 0\n",
        "bleu_syntax = 0\n",
        "bleu_no_copy_layer = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UALgDTleHdHd",
        "outputId": "c64aca9f-d8fa-438e-cd88-93a0fe632741"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    log_dict = {\"train_loss\": train_loss, \"valid_loss\": valid_loss}\n",
        "\n",
        "    if epoch % 10 == 0:# and epoch > 0:\n",
        "      bleu = batch_bleu(test_iterator, TRG, model, device)\n",
        "      print('bleu:', bleu)\n",
        "\n",
        "      if USE_COPY:\n",
        "        bleu_syntax = batch_bleu(test_iterator, BASE_TRG, model, device) # syntax\n",
        "        print('bleu syntax:', bleu_syntax)\n",
        "        bleu_no_copy_layer = batch_bleu(test_iterator, BASE_TRG, model, device, use_copy=False) # not using the copy layer\n",
        "        print('bleu no copy layer:', bleu_no_copy_layer)\n",
        "    \n",
        "    log_dict['bleu'] = bleu\n",
        "\n",
        "    if USE_COPY:\n",
        "      log_dict['bleu_syntax'] = bleu_syntax\n",
        "      log_dict['bleu_no_copy_layer'] = bleu_no_copy_layer\n",
        "\n",
        "    if USE_WANDBAI:\n",
        "      wandb.log(log_dict)\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(valid_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # save checkpoints like so:\n",
        "    # torch.save(model.state_dict(), f'./checkpoint-{epoch+1}.pt')\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'out/best-model-state-dict.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    # if there is a range error with the loss, make sure you are using SGD optimizer!\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293QwRGQ2N5o"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'out/current-state-dict.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ2HbQuFIw8f",
        "outputId": "7db68b1b-7144-425b-b186-b470fb373d77"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load(f'out/current-state-dict.pt'))\n",
        "model.load_state_dict(torch.load(f'out/best-model-state-dict.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "c3639d8adcf9434a86d10ce6b95a02b0",
            "02745abf1c0f4126998dba6a2cdbe48b",
            "c0330e62b59c428bbb31247b6549c605",
            "defeb89a54b64f5298d2f85582c1383d",
            "ae49129bedfe407f8602a9956eb8a543",
            "2abfca7ad7b2483aad70b37db8aa80ba",
            "9377577c7e8348d399e0f76ef234ddee",
            "c2ad17e9818e45ef94b652d0b3b1fc22"
          ]
        },
        "id": "wZEjcZP18PlY",
        "outputId": "29af1828-962d-4434-96b2-3c21d5217965"
      },
      "outputs": [],
      "source": [
        "# Finish wandbai logging\n",
        "if USE_WANDBAI:\n",
        "  wandb.finish(0, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "NLBWCQMDT_n9",
        "outputId": "3afec67e-9630-4eac-dd05-13f27d8bf9dd"
      },
      "outputs": [],
      "source": [
        "# Plot train and validation loss curves\n",
        "plt.plot(train_loss_list, label=\"Train loss\",  marker='o')\n",
        "plt.plot(val_loss_list, label = \"Validation loss\",  marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Loss curves')\n",
        "locs, labels = plt.xticks()\n",
        "plt.xticks(np.arange(0, len(train_loss_list)))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1CtdoM-BPVM",
        "outputId": "a35727a0-fdeb-4b7d-81fd-c950cb8cc151"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics from the best model (lowest loss, not always highest BLEU)\n",
        "\n",
        "model.load_state_dict(torch.load(f'out/best-model-state-dict.pt'))\n",
        "metrics = get_metrics(test_data, test_entries, SRC, TRG, model, device)\n",
        "\n",
        "for m in metrics:\n",
        "    print(f'{m} = {metrics[m]*100:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFCnx-I6HPpX"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8lgEnh8IxK5"
      },
      "outputs": [],
      "source": [
        "CONFIG_PATH = 'out/config.json'\n",
        "SRC_VOCAB_PATH = 'out/src_vocab.field'\n",
        "TRG_VOCAB_PATH = 'out/trg_vocab.field'\n",
        "MODEL_PATH = 'out/best-model-state-dict.pt'\n",
        "OOV_DATASET = 'oov_dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E94QMmKiJm9E"
      },
      "outputs": [],
      "source": [
        "# Translator class to facilitate inference, easier portability\n",
        "# If you use it as a standalone script, make sure to also import the following utils elements:\n",
        "# VocabDub, gen_data_field(), read_vocab(), extend_vocabulary(), translate_sentence()\n",
        "# As well as the model architecture:\n",
        "# Encoder, Decoder, CopyLayerVocabExtend and CNNSeq2Seq\n",
        "\n",
        "class Translator:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        with open(CONFIG_PATH, 'r') as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        self.src_pad_idx = config['SRC_PAD_IDX']\n",
        "        base_vocab_size = max(config['INPUT_DIM'], config['NUM_EMBEDDINGS'])\n",
        "        \n",
        "        # load vocab\n",
        "        self.SRC, self.TRG = gen_data_field()\n",
        "        self.SRC.build_vocab([], min_freq=1, max_size=None)\n",
        "        self.TRG.build_vocab([], min_freq=1, max_size=None)\n",
        "\n",
        "        src_vocab = VocabDup(read_vocab(SRC_VOCAB_PATH), base_vocab_size)\n",
        "        trg_vocab = VocabDup(read_vocab(TRG_VOCAB_PATH), base_vocab_size)\n",
        "\n",
        "        self.SRC = extend_vocabulary(self.SRC, src_vocab)\n",
        "        self.TRG = extend_vocabulary(self.TRG, trg_vocab)\n",
        "        \n",
        "        # safeguard bcs of backwards compat issues\n",
        "        dec_out_dim = config['DECODER']['OUT_DIM'] if 'OUT_DIM' in config['DECODER'] else DECODER_OUT_DIM\n",
        "\n",
        "        # define model\n",
        "        self.enc = Encoder(config['INPUT_DIM'], config['EMB_DIM'], config['HID_DIM'], config['ENCODER']['ENC_DROPOUT'], self.device, config['SRC_PAD_IDX'])\n",
        "        self.dec = Decoder(config['NUM_EMBEDDINGS'], dec_out_dim, config['EMB_DIM'], config['HID_DIM'], config['DECODER']['DEC_DROPOUT'], self.device, config['TRG_PAD_IDX'])\n",
        "        self.copy_layer = CopyLayerVocabExtend(self.dec) if config['USE_COPY'] else None\n",
        "        self.model = CNNSeq2Seq(self.enc, self.dec, self.copy_layer).to(self.device)\n",
        "\n",
        "        # load pretrained model\n",
        "        loaded = torch.load(MODEL_PATH)\n",
        "        self.model.load_state_dict(loaded)\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "    def translate(self, sentence: str) -> List[str]:\n",
        "      translation, _ = translate_sentence(\n",
        "          sentence.split(), self.SRC, self.TRG, self.model, self.device, predict_with_copy=self.model.copy_layer is not None)\n",
        "      \n",
        "      return translation\n",
        "\n",
        "    def calculate_bleu(self, test_data: List[Dict]) -> float:\n",
        "      print(\"Calculating BLEU score...\")\n",
        "\n",
        "      expected_trgs = []\n",
        "      pred_trgs = []\n",
        "      pred_copy_trgs = []\n",
        "      error_report = []\n",
        "\n",
        "      for entry in test_data:\n",
        "          src_sentence = entry['question']['interm_question'].lower()\n",
        "          trg_sentence = entry['query']['interm_sparql']\n",
        "\n",
        "          pred_trg = self.translate(src_sentence)\n",
        "          pred_trg = pred_trg[:-1]\n",
        "          pred_trgs.append(pred_trg)\n",
        "          expected_trgs.append([trg_sentence.split()])\n",
        "\n",
        "          error_entry = {\n",
        "            'id': entry['_id'],\n",
        "            'template_id': entry['template_id'],\n",
        "            'src': src_sentence,\n",
        "            'trg': trg_sentence,\n",
        "            'predicted': ' '.join(pred_trg),\n",
        "            'correct': trg_sentence == pred_trg\n",
        "          }\n",
        "          error_report.append(error_entry)\n",
        "\n",
        "      bleu = bleu_score(pred_trgs, expected_trgs)\n",
        "\n",
        "      with open('out/error_report_oov.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(error_report, f, indent=4)\n",
        "\n",
        "      return bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5Jl6gkLJn69",
        "outputId": "4f002e2a-9c29-4c1a-acbc-71c455572761"
      },
      "outputs": [],
      "source": [
        "translator = Translator()\n",
        "\n",
        "with open(OOV_DATASET, 'r', encoding='utf-8') as f:\n",
        "  dataset = json.load(f)\n",
        "  \n",
        "translator.calculate_bleu(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYwimVUWPq9"
      },
      "source": [
        "# SAVE ALL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pap230_kxOLk"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.move(\"out\", OUT_DRIVE_FOLDER)\n",
        "iteration += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bYrqjK7K-fi"
      },
      "outputs": [],
      "source": [
        "# DONE :)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uYwW2oyaw0Wh",
        "5B-4a0NCmjyG",
        "ylUzwqDDmn4t",
        "aeH21-kbmt_a",
        "5rSsdV20pefs"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02745abf1c0f4126998dba6a2cdbe48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae49129bedfe407f8602a9956eb8a543",
            "placeholder": "",
            "style": "IPY_MODEL_2abfca7ad7b2483aad70b37db8aa80ba",
            "value": "0.300 MB of 0.300 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "2abfca7ad7b2483aad70b37db8aa80ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9377577c7e8348d399e0f76ef234ddee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae49129bedfe407f8602a9956eb8a543": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0330e62b59c428bbb31247b6549c605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9377577c7e8348d399e0f76ef234ddee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2ad17e9818e45ef94b652d0b3b1fc22",
            "value": 1
          }
        },
        "c2ad17e9818e45ef94b652d0b3b1fc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3639d8adcf9434a86d10ce6b95a02b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02745abf1c0f4126998dba6a2cdbe48b",
              "IPY_MODEL_c0330e62b59c428bbb31247b6549c605"
            ],
            "layout": "IPY_MODEL_defeb89a54b64f5298d2f85582c1383d"
          }
        },
        "defeb89a54b64f5298d2f85582c1383d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
